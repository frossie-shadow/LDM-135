\documentclass[DM,lsstdraft,toc]{lsstdoc}

% lsstdoc documentation: https://lsst-texmf.lsst.io/lsstdoc.html

% Package imports go here.

% Local commands go here.

% To add a short-form title:
% \title[Short title]{Title}
\title{Data Management Database Design}

% Optional subtitle
% \setDocSubtitle{A subtitle}

\author{%
	Jacek Becla,
	Daniel Wang,
	Serge Monkewitz,
	K-T Lim,
	John Gates,
	Andy Salnikov,
	Andrew Hanushevsky,
	Douglas Smith,
	Bill Chickering,
	Michael Kelsey,
	and
	Fritz Mueller
}

\setDocRef{LDM-135}

\date{\today}

% Optional: name of the document's curator
% \setDocCurator{The Curator of this Document}

\setDocAbstract{%
This document discusses the LSST database system architecture.
}


% Change history defined here.
% Order: oldest first.
% Fields: VERSION, DATE, DESCRIPTION, OWNER NAME.
% See LPM-51 for version number policy.
\setDocChangeRecord{%
  \addtohist{1.0}{2009-06-15}{Initial version.}{Jacek Becla}
  \addtohist{2.0}{2011-07-12}{Most sections rewritten, added scalability test section}{Jacek Becla}
  \addtohist{2.1}{2011-08-12}{Refreshed future-plans and schedule of testing sections, added section about fault tolerance.}{Jacek Becla, Daniel Wang}
  \addtohist{3.0}{2013-08-02}{Synchronized with latest changes to the requirements \citedsp{LSE-163}. Rewrote most of the “Implementation” chapter. Documented new tests, refreshed all other chapters.}{Jacek Becla, Daniel Wang, Serge Monkewitz, Kian-Tat Lim, Douglas Smith, Bill Chickering}
  \addtohist{3.1}{2013-10-10}{Refreshed numbers based on latest \citeds{LDM-141}. Updated shared scans (implementation) and 300-node test sections, added section about shared scans demonstration}{Jacek Becla, Daniel Wang}
  \addtohist{3.2}{2013-10-10}{TCT approved}{R Allsman}
  \addtohist{}{2016-07-18}{Update with async query, shared scan, secondary index, xrootd, metadata service information.}{John Gates, Andy Salnikov, Andrew Hanushevsky, Michael Kelsey, Fritz Mueller}
  \addtohist{}{2017-07-05}{Move historical investigations to separate documents: \citeds{DMTN-046,DMTN-047,DMTN-048,DMTR-21,DMTR-12}}{T.~Jenness}
}

\begin{document}

% Create the title page.
% Table of contents is added automatically with the "toc" class option.
\maketitle

\section{Executive Summary}\label{executive-summary}

The LSST baseline database architecture for its massive user query
access system is an MPP (massively parallel processing) relational
database composed of a single-node non-parallel DBMS, a distributed
communications layer, and a master controller, all running on a
shared-nothing cluster of commodity servers with locally attached
spinning disk drives, capable of incremental scaling and recovering from
hardware failures without disrupting running queries. All large catalogs
are spatially partitioned horizontally into materialized \emph{chunks},
and the remaining catalogs are replicated on each server; the chunks are
distributed across all nodes. The Object catalog is further partitioned
into \emph{sub-chunks} with overlaps, materialized on-the-fly when
needed. Chunking is handled automatically without exposure to users.
Selected tables are also partitioned vertically to maximize performance
of most common analysis. The system uses a few critical indexes to speed
up spatial searches, time series analysis, and simple but interactive
queries. Shared scans are used to answer all but the interactive
queries. Such an architecture is primarily driven by the variety and
complexity of anticipated queries, ranging from single object lookups to
complex \(O(n^2)\) full-sky correlations over billions of elements.

The LSST baseline database architecture for its real time Alert
Production relies on horizontal time-based partitioning. To guarantee
reproducibility, no-overwrite-update techniques combined with
maintaining validity time for appropriate rows are employed. Two
database replicas are maintained to isolate live production catalogs
from user queries; the replicas are synchronized in real time using
native database replication.

Given the current state of the RDBMS and Map/Reduce market, an
RDBMS-based solution is a much better fit, primarily due to features
such as indexes, schema and speed. No off-the-shelf, reasonably priced
solution meets our requirements (today), even though production systems
at a scale comparable to LSST have been demonstrated already by
industrial users such as eBay using a prohibitively expensive,
commercial RDBMS.

The baseline design involves many choices such as component technology,
partition size, index usage, normalization level, compression
trade-offs, applicability of technologies such as solid state disks,
ingest techniques and others. We ran many tests to determine the design
configuration, determine limits and uncover potential bottlenecks. In
particular, we chose \emph{MySQL} as our baseline open source,
single-node DBMS and XRootD\footnote{\url{http://xrootd.org}} \citep{Dorigo:2005:XRootd}
as an open source, elastic, distributed, fault-tolerant messaging system.

We developed a prototype of the baseline architecture, called
\emph{Qserv}. To mitigate major risks, such as insufficient scalability
or potential future problems with the underlying RDBMS, Qserv pays close
attention to minimizing exposure to vendor-specific features and
add-ons. Many key features including the scalable dispatch system and
2-level partitioner have been implemented at the prototype level and
integrated with the two underlying production-quality components: MySQL
and XRootD. Scalability and performance have
been successfully demonstrated on a variety of clusters ranging from
20-node-100TB cluster to 300-node-30TB cluster, tables as large as 50
billion rows and concurrency exceeding 100,000 in-flight chunk-queries.
Required data rates for all types of queries (interactive, full sky
scans, joins, correlations) have been achieved. Basic fault tolerance
recovery mechanisms and basic version of shared scans were demonstrated.
Future work includes adding support for user tables, demonstrating
cross-matching, various non-critical optimizations, and most
importantly, making the prototype more user-friendly and turning it into
a production-ready system.

If an equivalent open-source, community supported, off-the-shelf
database system were to become available, it could present significant
support cost advantages over a production-ready Qserv. The largest
barrier preventing us from using an off-the-shelf system is spherical
geometry and spherical partitioning support.

To increase the chances such a system will become reality in the next
few years, in 2008 we initiated the SciDB array-based scientific
database project. Due to lack of many traditional RDBMS-features in
SciDB and still nascent fault tolerance, we believe it is easier to
build the LSST database system using
MySQL+XRootD than it would be to build it
based on SciDB. In addition we closely collaborate with the MonetDB open
source columnar database team -- a successful demonstration of Qserv
based on MonetDB instead of MySQL was done in 2012. Further, to stay
current with the state-of-the-art in peta-scale data management and
analysis, we continue a dialog with all relevant solution providers,
both DBMS and Map/Reduce, as well as with data-intensive users, both
industrial and scientific, through the XLDB\footnote{\url{https://xldb.org}}
conference series we lead, and beyond.

\section{Introduction}\label{introduction}

This document discusses the LSST database system architecture. \secref{baseline-architecture} discusses the baseline
architecture. \secref{requirements} explains the LSST database-related
requirements.
\secref{risk-analysis} covers risk analysis.
\secref{implementation} discusses the
prototype design (Qserv), including design, current status of the
software and future plans.
For some additional background, \citeds{DMTN-046} covers in-depth analysis
of off-the-shelf potential solutions (Map/Reduce and RDBMS) as of 2013,
and \citeds{DMTR-21} and \citeds{DMTR-12} describe large scale Qserv tests from 2013.
The full Qserv test specification is described in \citeds{LDM-552}.
\citeds{DMTN-048} discusses the original design trade-offs
and decision process, including small scale tests that were run and some Qserv demonstrations.

\section{Baseline Architecture}\label{baseline-architecture}

This section describes the most important aspects of the LSST baseline
database architecture. The choice of the architecture is driven by the
project requirements (see reqs) as well as cost, availability and
maturity of the off-the-shelf solutions currently available on the
market (see \citeds{DMTN-046}), and design trade-offs (see
\citeds{DMTN-048}). The architecture is periodically revisited: we
continuously monitor all relevant technologies, and accordingly
fine-tune the baseline architecture.

In summary, the LSST baseline architecture for Alert Production is an
off-the-shelf RDBMS system which uses replication for fault tolerance
and which takes advantage of horizontal (time-based) partitioning. The
baseline architecture for user access to Data Releases is an MPP
(multi-processor, parallel) relational database running on a
shared-nothing cluster of commodity servers with locally attached
spinning disk drives; capable of (a) incremental scaling and (b)
recovering from hardware failures without disrupting running queries.
All large catalogs are spatially partitioned into materialized
\emph{chunks}, and the remaining catalogs are replicated on each server;
the chunks are distributed across all nodes. The Object catalog is
further partitioned into \emph{sub-chunks} with overlaps,\footnote{A
  chunk's overlap is implicitly contained within the overlaps of its
  edge sub-chunks.} materialized on-the-fly when needed. Shared scans
are used to answer all but low-volume user queries. Details follow
below.

\subsection{Alert Production and Up-to-date
Catalog}\label{alert-production-and-up-to-date-catalog}

Alert Production involves detection and measurement of
difference-image-analysis sources (DiaSources). New DiaSources are
spatially matched against the most recent versions of existing
DiaObjects, which contain summary properties for variable and transient
objects (and false positives). Unmatched DiaSources are used to create
new DiaObjects. If a DiaObject has an associated DiaSource that is no
more than a month old, then a forced measurement (DiaForcedSource) is
taken at the position of that object, whether a corresponding DiaSource
was detected in the exposure or not.

The output of Alert Production consists mainly of three large catalogs
-- DiaObject, DiaSource, and DiaForcedSource - as well as several
smaller tables that capture information about e.g. exposures, visits and
provenance.

These catalogs will be modified live every night. After Data Release
Production has been run based on the first six months of data and each
year's data thereafter, the live Level 1 catalogs will be archived to
tape and replaced by the catalogs produced by DRP. The archived catalogs
will remain available for bulk download, but not for queries.

Note that existing DiaObjects are never overwritten. Instead, new
versions of the AP-produced and DRP-produced DiaObjects are inserted,
allowing users to retrieve (for example) the properties of DiaObjects as
known to the pipeline when alerts were issued against them. To enable
historical queries, each DiaObject row is tagged with a validity start
and end time. The start time of a new DiaObject version is set to the
observation time of the DiaSource or DiaForcedSource that led to its
creation, and the end time is set to infinity. If a prior version
exists, then its validity end time is updated (in place) to equal the
start time of the new version. As a result, the most recent versions of
DiaObjects can always be retrieved with:

\begin{lstlisting}[language=SQL]
SELECT * FROM DiaObject WHERE validityEnd = infinity
\end{lstlisting}

Versions as of some time \emph{t} are retrievable via:

\begin{lstlisting}[language=SQL]
SELECT * FROM DiaObject WHERE validityStart <= t AND t < validityEnd
\end{lstlisting}

Note that a DiaSource can also be reassociated to a solar-system object
during day time processing. This will result in a new DiaObject version
unless the DiaObject no longer has any associated DiaSources. In that
case, the validity end time of the existing version is set to the time
at which the reassociation occurred.

Once a DiaSource is associated with a solar system object, it is never
associated back to a DiaObject. Therefore, rather than also versioning
DiaSources, columns for the IDs of both the associated DiaObject and
solar system object, as well as a reassociation time, are included.
Reassociation will set the solar system object ID and reassociation
time, so that DiaSources for DiaObject 123 at time \emph{t} can be
obtained using:

\begin{lstlisting}[language=SQL]
SELECT *
FROM DiaSource
WHERE diaObjectId = 123
AND midPointTai <= t
AND (ssObjectId is NULL OR ssObjectReassocTime > t)
\end{lstlisting}

DiaForcedSources are never reassociated or updated in any way.

From the database point of view then, the alert production pipeline will
perform the following database operations 189 times (once per LSST CCD)
per visit (every 39 seconds):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Issue a point-in-region query against the DiaObject catalog, returning
  the most recent versions of the objects falling inside the CCD.
\item
  Use the IDs of these diaObjects to retrieve all associated diaSources
  and diaForcedSources.
\item
  Insert new diaSources and diaForcedSources.
\item
  Update validity end times of diaObjects that will be superseded.
\item
  Insert new versions of diaObjects.
\end{enumerate}

All spatial joins will be performed on in-memory data by pipeline code,
rather than in the database. While Alert Production does also involve a
spatial join against the Level 2 (DRP-produced) Object catalog, this
does not require any database interaction: Level 2 Objects are never
modified, so the Object columns required for spatial matching will be
dumped to compact binary files once per Data Release. These files will
be laid out in a way that allows for very fast region queries, allowing
the database to be bypassed entirely.

The DiaSource and DiaForcedSource tables will be split into two tables,
one for historical data and one containing records inserted during the
current night. The current-night tables will be small and rely on a
transactional engine like InnoDB, allowing for speedy recovery from
failures. The historical-data tables will use the faster
non-transactional MyISAM or Aria storage engine, and will also take
advantage of partitioning. The Data Release catalogs used to seed the
live catalogs will be stored in a single initial partition, sorted
spatially (using the Hierarchical Triangular Mesh trixel IDs for their
positions). This means that the diaSources and diaForcedSources for the
diaObjects in a CCD will be located close together on disk, minimizing
seeks. Every month of new data will be stored in a fresh partition,
again sorted spatially. Such partitions will grow to contain just a few
billion rows over the course of a month, even for the largest catalog.
At the end of each night, the contents of the current-night table are
sorted and appended to the partition for the current-month, then
emptied. Each month, the entire current-month partition is sorted
spatially (during the day), and a partition for the next month is
created.

For DiaObject, the same approach is used. However, DiaObject validity
end-time updates can occur in any partition, and are not confined to the
current-night table. We therefore expect to use a transactional storage
engine like InnoDB for all partitions. Because InnoDB clusters tables
using the primary key, we will likely declare it to consist of a leading
HTM ID column, followed by disambiguating columns (diaObjectId,
validityStart). The validity end time column will not be part of any
index.

No user queries will be allowed on the live production catalogs. We
expect to maintain a separate replica just for user queries,
synchronized in real time using one-way master-slave native database
replication. The catalogs for user queries will be structured
identically to the live catalogs, and views will be used to hide the
splits (using a ``\texttt{UNION\ ALL}'').

For additional safety, we might choose to replicate the small
current-night tables, all DiaObject partitions, and the remaining
(small) changing tables to another hot stand-by replica. In case of
disastrous master failure that cannot be fixed rapidly, the slave
serving user queries will be used as a temporary replacement, and user
queries will be disallowed until the problem is resolved.

Based on the science requirements, only short-running, relatively simple
user queries will be needed on the Level 1 catalogs. The most complex
queries, such as large-area near neighbor queries, will not be needed.
Instead, user queries will consist mainly of small-area cone searches,
light curve lookups, and historical versions of the same. Since the
catalogs are sorted spatially, we expect to be able to quickly answer
spatial queries using indexed HTM ID columns and the scisql UDFs, an
approach that has worked well in data-challenges to date. Furthermore,
note that the positions of diaSources/diaForcedSources associated with
the same diaObject will be very close together, so that sorting to
obtain good spatial locality also ends up placing sources belonging to
the same light curve close together. In other words, the data
organization used to provide fast pipeline query response is also
advantageous for user queries.

\subsection{Data Release Production}\label{data-release-production}

Data Release Production will involve the generation of significantly
larger catalogs than Alert Production. However, these are produced over
the course of several months, pipelines will not write directly to the
database, and there are no pipeline queries with very low-latency
execution time requirements to be satisfied. While we do expect several
pipeline-related full table scans over the course of a Data Release
Production, we will need to satisfy many user queries involving such
scans on a daily basis. User query access is therefore the primary
driver of our scalable database architecture, which is described in
detail below. For a description of the data loading process, please see
qserve-data-loading.

\subsection{User Query Access}\label{user-query-access}

The user query access is the primary driver of the scalable database
architecture. Such architecture is described below.

\subsubsection{Distributed and parallel}\label{distributed-and-parallel}

The database architecture for user query access relies on a model of
distributing computation among autonomous worker nodes. Autonomous
workers have no direct knowledge of each other and can complete their
assigned work without data or management from their peers. This implies
that data must be partitioned, and the system must be capable of
dividing a single user query into sub-queries, and executing these
sub-queries in parallel -- running a high-volume query without
parallelizing it would take unacceptably long time, even if run on very
fast CPU. The parallelism and data distribution should be handled
automatically by the system and hidden from users.

\subsubsection{Shared-nothing}\label{shared-nothing}

Such architecture provides good foundation for incremental scaling and
fault recovery: because nodes have no direct knowledge of each other and
can complete their assigned work without data or management from their
peers, it is possible to add node to, or remove node from such system
with no (or with minimal) disruption. However, to achieve fault
tolerance and provide recover mechanisms, appropriate smarts have to be
build into the node management software.

\begin{figure}[H]
\centering
\includegraphics{_static/shared_nothing_arch.jpg}
\caption{Shared-nothing database architecture.}
\end{figure}

\subsubsection{Indexing}\label{indexing}

Disk I/O bandwidth is expected to be the greatest bottleneck. Data can
be accessed either through index, which typically translates to a random
access, or a scan, which translates to a sequential read (unless
multiple competing scans are involved).

Indexes dramatically speed up locating individual rows, and avoid
expensive full table scans. They are essential to answer low volume
queries quickly, and to do efficient table joins. Also, spatial indexes
are essential. However, unlike in traditional, small-scale systems, the
advantages of indexes become questionable when a larger number of rows
is to be selected from a table. In case of LSST, selecting even a 0.01\%
of a table might lead to selecting millions of rows. Since each fetch
through an index might turn into a disk seek, it is often cheaper to
read sequentially from disk than to seek for particular rows via index,
especially when the index itself is out-of-memory. For that reason the
architecture forgoes relying on heavy indexing, only a small number of
carefully selected indexes essential for answering low-volume queries,
enabling table joins, and speeding up spatial searches will be
maintained. For an analytical query system, it makes sense to make as
few assumptions as possible about what will be important to our users,
and to try and provide reasonable performance for as broad a query load
as possible, i.e. focus on scan throughput rather than optimizing
indexes. A further benefit to this approach is that many different
queries are likely to be able to share scan IO, boosting system
throughput, whereas caching index lookup results is likely to provide
far fewer opportunities for sharing as the query count scales (for the
amounts of cache we can afford).

\subsubsection{Shared scanning}\label{shared-scanning}

Now with table-scanning being the norm rather than the exception and
each scan taking a significant amount of time, multiple full-scan
queries would randomize disk access if they each employed their own
full-scanning read from disk. Shared scanning (also called \emph{convoy
scheduling}) shares the I/O from each scan with multiple queries. The
table is read in pieces, and all concerning queries operate on that
piece while it is in memory. In this way, results from many full-scan
queries can be returned in little more than the time for a single
full-scan query. Shared scanning also lowers the cost of data
compression by amortizing the CPU cost among the sharing queries,
tilting the tradeoff of increased CPU cost versus reduced I/O cost
heavily in favor of compression.

Shared scanning will be used for all high-volume and super-high volume
queries. Shared scanning is helpful for unpredictable, ad-hoc analysis,
where it prevents the extra load from increasing the disk /IO cost --
only more CPU is needed. On average we expect to continuously run the
following scans:

\begin{itemize}
\item
  one full table scan of Object table for the latest data release only,
\item
  one synchronized full table scan of Object, Source and ForcedSource
  tables every 12 hours for the latest data release only,
\item
  one synchronized full table scan of Object and Object\_Extra every 8
  hours for the latest and previous data releases.
\end{itemize}

Appropriate Level 3 user tables will be scanned as part of each shared
scan as needed to answer any in-flight user queries.

Shared scans will take advantage of table chunking explained below. In
practice, within a single node a scan will involve fetching sequentially
a chunk of data at a time and executing on this chunk all queries in the
queue. The level of parallelism will depend on the number of available
cores.

Running multiple shared scans allows relatively fast response time for
Object-only queries, and supporting complex, multi-table joins:
synchronized scans are required for two-way joins between different
tables. For a self-joins, a single shared scans will be sufficient,
however each node must have sufficient memory to hold 2 chunks at any
given time (the processed chunk and next chunk). Refer to the sizing
model \citedsp{LDM-141} for further details on the cost of shared scans.

Low-volume queries will be executed ad-hoc, interleaved with the shared
scans. Given the number of spinning disks is much larger than the number
of low-volume queries running at any given time, this will have very
limited impact on the sequential disk I/O of the scans, as shown in
\citeds{LDM-141}.

\subsubsection{Clustering}\label{clustering}

The data in the Object Catalog will be physically clustered on disk
spatially -- that means that objects collocated in space will be also
collocated on disk. All Source-type catalogs (Source, ForcedSource,
DiaSource, DiaForcedSource) will be clustered based on their
corresponding objectId -- this approach enforces spatial clustering and
collocates sources belonging to the same object, allowing sequential
read for queries that involve times series analysis.

SSObject catalog will be unpartitioned, because there is no obvious
fixed position that we could choose to use for partitioning. The
associated diaSources (which will be intermixed with diaSources
associated with static diaSources) will be partitioned, according their
position. For that reason the SSObject-to-DiaSource join queries will
require index searches on all chunks, unlike DiaObject-to-DiaSource
queries. Since SSObject is small (low millions), this should not be an
issue.

\subsubsection{Partitioning}\label{partitioning}

Data must be partitioned among nodes in a shared-nothing architecture.
While some \emph{sharding} approaches partition data based on a hash of
the primary key, this approach is unusable for LSST data since it
eliminates optimizations based on celestial objects' spatial nature.

\paragraph{Sharded data and sharded
queries}\label{sharded-data-and-sharded-queries}

All catalogs that require spatial partitioning (Object, Source,
ForcedSource, DiaSource, DiaForcedSource) as well as all the auxiliary
tables associated with them, such as ObjectType, or PhotoZ, will be
divided into spatial partitions of roughly the same area by partitioning
then into \emph{declination} zones, and chunking each zone into
\emph{RA} stripes. Further, to be able to perform table joins without
expensive inter-node data transfers, partitioning boundaries for each
partitioned table must be aligned, and chunks of different tables
corresponding to the same area of sky must be co-located on the same
node. To ensure chunks are appropriately sized, the two largest
catalogs, Source and ForcedSource, are expected to be partitioned into
finer-grain chunks. Since objects occur at an approximately-constant
density throughout the celestial sphere, an equal-area partition should
spread a load that is uniformly distributed over the sky.

Smaller catalogs that can be partitioned spatially, such as Alert and
exposure metadata will be partitioned spatially. All remaining catalogs,
such provenance or SDQA tables will be replicated on each node. The size
of these catalogs is expected to be only a few terabytes.

With data in separate physical partitions, user queries are themselves
fragmented into separate physical queries to be executed on partitions.
Each physical query's result can be combined into a single final result.

\paragraph{Two-level partitions}\label{two-level-partitions}

Determining the size and number of data partitions may not be obvious.
Queries are fragmented according to partitions so an increasing number
of partitions increases the number of physical queries to be dispatched,
managed, and aggregated. Thus a greater number of partitions increases
the potential for parallelism but also increases the overhead. For a
data-intensive and bandwidth-limited query, a parallelization width
close to the number of disk spindles should minimize seeks and
maximizing bandwidth and performance.

From a management perspective, more partitions facilitate rebalancing
data among nodes when nodes are added or removed. If the number of
partitions were equal to the number of nodes, then the addition of a new
node would require the data to be re-partitioned. On the other hand, if
there were many more partitions than nodes, then a set of partitions
could be assigned to the new node without re-computing partition
boundaries.

Smaller and more numerous partitions benefit spatial joins. In an
astronomical context, we are interested in objects near other objects,
and thus a full \(O(n^2)\) join is not required--a localized spatial
join is more appropriate. With spatial data split into smaller
partitions, an SQL engine computing the join need not even consider (and
reject) all possible pairs of objects, merely all the pairs within a
region. Thus a task that is \(O(n^2)\) naively becomes \(O(kn)\) where
\(k\) is the number of objects in a partition.

In consideration of these trade-offs, two-level partitioning seems to be
a conceptually simple way to blend the advantages of both extremes.
Queries can be fragmented in terms of coarse partitions (``chunks''),
and spatial near-neighbor joins can be executed over more fine
partitions (``sub-chunks'') within each partition. To avoid the overhead
of the sub-chunks for non-join queries, the system can store chunks and
generate sub-chunks on-demand for spatial join queries. On-the-fly
generation for joins is cost-effective due to the drastic reduction of
pairs, which is true as long as there are many sub-chunks for each
chunk.

\paragraph{Overlap}\label{overlap}

A strict partitioning eliminates nearby pairs where objects from
adjacent partitions are paired. To produce correct results under strict
partitioning, nodes need access to objects from outside partitions,
which means that data exchange is required. To avoid this, each
partition can be stored with a pre-computed amount of overlapping data.
This overlapping data does not strictly belong to the partition but is
within a preset spatial distance from the partition's borders. Using
this data, spatial joins can be computed correctly within the preset
distance without needing data from other partitions that may be on other
nodes.

Overlap is needed only for the Object Catalog, as all spatial
correlations will be run on that catalog only. Guided by the experience
from other projects including SDSS, we expect to preset the overlap to
\textasciitilde{}1 arcmin, which results in duplicating approximately
30\% of the Object Catalog.

\paragraph{Spherical geometry}\label{spherical-geometry}

Support for spherical geometry is not common among databases and
spherical geometry-based partitioning was non-existent in other
solutions when we decided to develop Qserv. Since spherical geometry is
the norm in recording positions of celestial objects (right-ascension
and declination), any spatial partitioning scheme for astronomical
object must account for its complexities.

\paragraph{Data immutability}\label{data-immutability}

It is important to note that user query access operates on read-only
data. Not having to deal with updates simplifies the architecture and
allows us to add extra optimizations not possible otherwise. The Level 1
data which is updated is small enough and will not require the scalable
architecture -- we plan to handle all Level 1 data set with out-of-the
box MySQL as described in \secref{alert-production-and-up-to-date-catalog}.

\subsubsection{Long-running queries}\label{long-running-queries}

Many of the typical user queries may need significant time to complete,
at the scale of hours. To avoid re-submission of those long-running
queries in case of various failures (networking or hardware issues) the
system will support asynchronous query execution mode. In this mode
users will submit queries using special options or syntax and the system
will dispatch a query and immediately return to user some identifier of
the submitted query without blocking user session. This query identifier
will be used by user to retirieve query processing status, query result
after query completes, or a partial query result while query is still
executing.

The system should be able to estimate the time which user query will
need to complete and refuse to run long queries in a regular blocking
mode.

\subsubsection{Technology choice}\label{technology-choice}

As explained in \citeds{DMTN-046},
no off-the-shelf solution meets the above requirements today, and RDBMS
is a much better fit than Map/Reduce-based system, primarily due to
features such as indexes, schema and speed. For that reason, our
baseline architecture consists of \emph{custom} software built on two
production components: an open source, ``simple'', single-node,
non-parallel DBMS (MySQL) and XRootD. To ease
potential future DBMS migrations, the communication with the underlying
DBMS relies on \emph{basic} DBMS functionality only, and avoids any
vendor-specific features and additions.

\begin{figure}[H]
\centering
\includegraphics{_static/qserve_components.png}
\caption{Component connections in Qserv.}
\end{figure}

\section{Requirements}\label{requirements}

The key requirements driving the LSST database architecture include:
incremental scaling, near-real-time response time for ad-hoc simple user
queries, fast turnaround for full-sky scans/correlations, reliability,
and low cost, all at multi-petabyte scale. These requirements are
primarily driven by the ad-hoc user query access.

\subsection{General Requirements}\label{general-requirements}

\textbf{Incremental scaling}. The system must to tens of petabytes and
trillions of rows. It must grow as the data grows and as the access
requirements grow. New technologies that become available during the
life of the system must be able to be incorporated easily. Expected
sizes for the largest database catalogs (for the last data release,
uncompressed, data only) are captured in
the table below \textless{}tab-expected-catalog-size\textgreater{}. For
further storage, disk and network bandwidth and I/O analyses, see
\citeds{LDM-141}.

\textbf{Reliability}. The system must not lose data, and it must provide
at least 98\% up time in the face of hardware failures, software
failures, system maintenance, and upgrades.

\textbf{Low cost}. It is essential to not overrun the allocated budget,
thus a cost-effective, preferably open-source solution is strongly
preferred.

\subsection{Data Production Related
Requirements}\label{data-production-related-requirements}

In a nutshell, the LSST database catalogs will be generated by a small
set of production pipelines:

\begin{itemize}
\item
  Data Release Production -- it produces all key catalogs. Ingest rates
  are very modest, as DRP takes several months to complete and is
  dominated by CPU-intensive application jobs. Ingest can be done
  separately from pipeline processing, as an post-processing step.
\item
  Nightly Alert Production -- it produces difference image sources, and
  updates the DiaObject, SSObject, DiaSource, DiaForcedSource catalogs.
  Since alerts need to be generated in under a minute after data has
  been taken, data has to be ingested/updated in almost-real time. The
  number of row updates/ingested is modest: \textasciitilde{}40K new
  rows and updates occur every \textasciitilde{}39 sec \citep{2008ASPC..394..114B}.
\item
  Calibration Pipeline -- it produces calibration information. Due to
  small data volume and no stringent timing requirements, ingest
  bandwidth needs are very modest.
\end{itemize}

In addition, the camera and telescope configuration is captured in the
Engineering \& Facility Database. Data volumes are very modest.

Further, the Level 1 live catalog will need to be updated with minimal
delay. This catalog should not be taken off-line for extended periods of
time.

The database system must allow for occasional schema changes for the
Level 1 data, and occasional changes that do not alter query
results\footnote{Example of non-altering changes including
  adding/removing/resorting indexes, adding a new column with derived
  information, changing type of a column without loosing information,
  (eg., \texttt{FLOAT} to \texttt{DOUBLE} would be always allowed,
  DOUBLE to \texttt{FLOAT} would only be allowed if all values can be
  expressed using \texttt{FLOAT} without loosing any information)} for
the Level 2 data after the data has been released. Schemas for different
data releases are allowed to be very different.

\subsection{Query Access Related
Requirements}\label{query-access-related-requirements}

The Science Data Archive Data Release query load is defined primarily in
terms of access to the large catalogs in the archive: Object, Source,
and ForcedSource. Queries to image metadata, for example, though
numerous, are expected to be fast and can easily be handled by
replicating the relatively small metadata tables.

The large catalog query load is specified as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  100 simultaneous queries for rows corresponding to single Objects or
  small spatial regions (on the order of at most 10s of arcminutes),
  with each query having an average response time of 10 seconds. This
  leads to a throughput of 10 ``low-volume'' queries per second. This
  number is approximately five times the peak ``professional
  astronomer'' query rate to the SDSS SkyServer. Each low-volume query
  is expected to return 0.5 GB of data or less. These queries are
  further subdivided as follows:

  \begin{enumerate}
  \def\labelenumii{\Alph{enumii}.}
  \item
    Single object fetches: 5\%.
  \item
    Few objects fetched by objectId: 60\%.
  \item
    Small area by spatial index: 25\%.
  \item
    Small area by scan: 10\%.
  \end{enumerate}

  Furthermore, 70\% of queries are expected to be of Objects only, with
  20\% retrieving Sources for Objects and 10\% retrieving ForcedSources.
\item
  50 simultaneous analytical queries involving full table scans of one
  or more large tables, with a target throughput of 20 queries per hour.
  Each ``high-volume'' query is expected to return up to 6 GB of data.
  We further subdivide these as follows:

  \begin{enumerate}
  \def\labelenumii{\Alph{enumii}.}
  \item
    Throughput of 16 queries per hour with an average latency of 1 hour
    on the most frequently accessed columns in the Object table. These
    provide fast turnaround and high throughput for the most common
    types of queries.
  \item
    Throughput of 1 query per hour with average latency of 12 hours for
    joins of the Source table with the most frequently accessed columns
    in the Object table. These provide a reasonable turnaround time and
    good throughput for time series queries.
  \item
    Throughput of 1 query per hour with average latency of 12 hours for
    joins of the ForcedSource table with the most frequently accessed
    columns in the Object table. These provide a reasonable turnaround
    time and good throughput for detailed time series queries.
  \item
    Throughput of 1 query per hour with average latency of 8 hours for
    scans of the full Object table or joins of it with up to three
    additional tables other than Source and ForcedSource. These provide
    ``adhoc'' access for complex queries.
  \item
    Throughput of 1 query per hour with average latency of 8 hours for
    scans of the full Object table in the previous Data Release or joins
    of it with up to three additional tables. These provide ``ad hoc''
    access for older data.
  \end{enumerate}
\end{enumerate}

We also include in the requirements up to 20 simultaneous queries for
the Level 1 Database and 5 simultaneous queries for the Engineering and
Facilities Database, both completing in an average of 10 seconds.

\textbf{Reproducibility}. Queries executed on any Level 1 and Level 2
data products must be reproducible.

\textbf{Real time}. A large fraction of ad-hoc user access will involve
so called ``low-volume'' queries -- queries that touch small area of
sky, or request small number of objects. These queries are required to
be answered in under 10 sec. On average, we expect to see
\textasciitilde{}100 such queries running at any given time.

\textbf{Fast turnaround}. High-volume queries -- queries that involve
full-sky scans are expected to be answered in 1 hour , while more
complex full-sky spatial and temporal correlations are expected to be
answered in \textasciitilde{}8-12 hours. \textasciitilde{}50
simultaneous high-volume queries are expected to be running at any given
time.

\textbf{Cross-matching with external/user data}. Occasionally, LSST
database catalog will need to be cross-matched with external catalogs:
both large, such as SDSS, SKA or GAIA, and small, such as small amateur
data sets. Users should be able to save results of their queries, and
access them during subsequent queries.

\textbf{Query complexity}. The system needs to handle complex queries,
including spatial correlations, time series comparisons. Spatial
correlations are required for the Object catalog only -- this is an
important observation, as this class of queries requires highly
specialized, 2-level partitioning with overlaps.

\textbf{Ad-hoc}. It is impossible to predict all types of analysis
astronomers will run. The unprecedented volume and scope of data might
enable new kind of analysis, and new ways of analysis.

\textbf{Flexibility}. Sophisticated end users need to be able to access
all this data in a flexible way with as few constraints as possible.
Many end users will want to express queries directly in SQL, most of
basic SQL92 will be required. It is not yet clear whether the full
language is necessary or if a subset is adequate, and, if so, what
operations need to be part of that subset.

\subsection{Discussion}\label{discussion}

\subsubsection{Implications}\label{implications}

The above requirements have important implications on the LSST data
access architecture.

\begin{itemize}
\item
  The system must allow rapid selection of small number of rows out of
  multi-billion row tables. To achieve this, efficient data indexing in
  both spatial and temporal dimensions is essential.
\item
  The system must efficiently join multi-trillion with multi-billion row
  tables. Denormalizing these tables to avoid common joins, such as
  Object with Source or Object with ForcedSource, would be prohibitively
  expensive.
\item
  The system must provide high data bandwidth. In order to process
  terabytes of data in minutes, data bandwidths on the order of tens to
  hundreds of gigabytes per second are required.
\item
  To achieve high bandwidths, to enable expandability, and to provide
  fault tolerance, the system will need to run on a distributed cluster
  composed of multiple machines.
\item
  The most effective way to provide high-bandwidth access to large
  amounts of data is to partition the data, allowing multiple machines
  to work against distinct partitions. Data partitioning is also
  important to speed up some operations on tables, such as index
  building.
\item
  Multiple machines and partitioned data in turn imply that at least the
  largest queries will be executed in parallel, requiring the management
  and synchronization of multiple tasks.
\item
  Limited budget implies the system needs to get most out available
  hardware, and scale it incrementally as needed. The system will be
  disk I/O limited, and therefore we anticipate attaching multiple
  queries to a single table scan (shared scans) will be a must.
\end{itemize}

\subsubsection{Query complexity and access
patterns}\label{query-complexity-and-access-patterns}

A compilation of representative queries provided by the LSST Science
Collaborations, the Science Council, and other surveys have been
captured \citep{CommonQueries}. These queries can be divided into
several distinct groups: analysis of a single object, analysis of
objects meeting certain criteria in a region or across entire sky,
analysis of objects close to other objects, analysis that require
special grouping, time series analysis and cross match with external
catalogs. They give hints as to the complexity required: these queries
include distance calculations, spatially localized self-joins, and time
series analysis.

Small queries are expected to exhibit substantial spatial locality
(refer to rows that contain similar spatial coordinates: right ascension
and declination). Some kinds of large queries are expected to exhibit a
slightly different form of spatial locality: joins will be among rows
that have nearby spatial coordinates. Spatial correlations will be
executed on the Object table; spatial correlations will \emph{not} be
needed on Source or ForcedSource tables.

Queries related to time series analysis are expected to need to look at
the history of observations for a given Object, so the appropriate
Source or ForcedSource rows must be easily joined and aggregate
functions operating over the list of Sources must be provided.

External data sets and user data, including results from past queries
may have to be distributed alongside distributed production table to
provide adequate join performance.

The query complexity has important implications on the overall
architecture of the entire system.

\section{Risk Analysis}\label{risk-analysis}

\subsection{Potential Key Risks}\label{potential-key-risks}

Insufficient \textbf{database performance and scalability} is one of the
major risks \citedsp{Document-7025}.

We have a prototype system (\emph{Qserv}) that will be turned into a
production system. Given that a large fraction of its functionality is
derived from two stable, production quality, open source components
(MySQL and XRootD), turning it into production
system is possible during the LSST construction phase.

A viable alternative might be to use an off-the-shelf system. In fact,
an off-the-shelf solution could present significant support cost
advantages over a production-ready Qserv, especially if it is a system
well supported by a large user and developer community. It is likely
that an open source, scalable solution will be available on the time
scale needed by LSST (for the beginning of LSST construction a stable
beta would suffice, beginning of production scalability approaching few
hundred terabytes would be sufficient). Database systems larger than the
largest single LSST data set have been successfully demonstrated in
production today. For example, eBay manages a 10+ petabyte production
database \citep{Monash:2010:ebay} and expects to deploy a 36 petabyte system
later in 2011. For comparison, the largest single LSST data set,
including all indexes and overheads is expected to be below 10 petabytes
in size, and will be produced \textasciitilde{}20 years from now (the
last Data Release).\footnote{The numbers, both for eBay and LSST are for
  compressed data sets.} The eBay system is based on an expensive
commercial DBMS (Teradata), but there is a growing demand for large
scale systems and growing competition in that area (Hadoop, SciDB,
Greenplum, InfiniDB, MonetDB, Caché and others).

Finally, a third alternative would be to use a closed-source, non free
software, such as Caché, InfiniDB or Greenplum (Teradata is too
expensive). Some of these systems, in particular Caché and InfiniDB are
very reasonably priced. We believe the largest barrier preventing us
from using an off-the-shelf DBMS such as InfiniDB is spherical geometry
and spherical partitioning support.

Potential \textbf{problems with off-the-shelf database software} used,
such as MySQL is another potential risk. MySQL has recently been
purchased by Oracle, leading to doubts as to whether the MySQL project
will be sufficiently supported in the long-term. Since the purchase,
several independent forks of MySQL software have emerged, including
MariaDB (supported by one of the MySQL founders),
Drizzle\footnote{Now abandoned:
\url{https://en.wikipedia.org/wiki/Drizzle_\%28database_server\%29}}
(supported by key architects of
MySQL), and Percona. Should MySQL disappear, these open-source,
MySQL-compatible\footnote{With the exception of
  Drizzle, which introduced major changes to
  the architecture.} systems are a solid alternative. Should we need to
migrate to a different DBMS, we have taken multiple measures to minimize
the impact:

\begin{itemize}
\item
  our schema does not contain any MySQL-specific elements and we have
  successfully demonstrating using it in other systems such as MonetDB
  and Microsoft's SQL Server;
\item
  we do not rely on any MySQL specific extensions, with the exception of
  MySQL Proxy, which can be made to work with non-MySQL systems if
  needed;
\item
  we minimize the use of stored functions and stored procedures which
  tend to be DBMS-specific, and instead use user defined functions,
  which are easier to port (only the interface binding part needs to be
  migrated).
\end{itemize}

\textbf{Complex data analysis}. The most complex analysis we identified
so far include spatial and temporal correlations which exhibit
\(O(n^2)\) performance characteristics, searching for anomalies and rare
events, as well as searching for unknown are a risk as well -- in most
cases industrial users deal with much simpler, well defined access
patters. Also, some analysis will be ad-hoc, and access patterns might
be different than these we are anticipating. Recently, large-scale
industrial users started to express strong need for similar types of
analyses; understanding and correlating user behavior (time-series of
user clicks) run by web companies, searching for abnormal user behavior
to detect fraud activities run by banks and web companies, analyzing
genome sequencing data run by biotech companies, and what-if market
analysis run by financial companies are just a few examples. Typically
these analysis are ad-hoc and involve searching for unknowns, similar to
scientific analyses. As the demand (by rich, industrial users) for this
type of complex analyses grows, the solution providers are rapidly
starting to add needed features into their systems.

The complete list of all database-related risks maintained in the LSST
risk registry:

\begin{itemize}
\item
  DM-014: Database performance insufficient for planned load
\item
  DM-015: Unexpected database access patterns from science users
\item
  DM-016: Unexpected database access patterns from DM productions
\item
  DM-032: LSST DM hardware architecture becomes antiquated
\item
  DM-060: Dependencies on external software packages
\item
  DM-061: Provenance capture inadequate
\item
  DM-065: LSST DM software architecture incompatible with de-facto
  community standards
\item
  DM-070: Archive sizing inadequate
\item
  DM-074: LSST DM software architecture becomes antiquated
\item
  DM-075: New SRD requirements require new DM functionality
\end{itemize}

\subsection{Risks Mitigations}\label{risks-mitigations}

To mitigate the insufficient performance/scalability risk, we developed
Qserv, and demonstrated scalability and performance. In addition, to
increase chances an equivalent open-source, community supported,
off-the-shelf database system becomes available in the next few years,
we initiated the SciDB array-based scientific database project and work
closely with its development team. We also closely collaborate with the
MonetDB open source columnar database team -- building on our Qserv
lessons-learned, they are trying to add missing features and turn their
software into a system capable of supporting LSST needs. A demonstration
is expected in late 2011. Further, to stay current with the
state-of-the-art in peta-scale data management and analysis, we continue
a dialog with all relevant solution providers, both DBMS and Map/Reduce,
as well as with data-intensive users, both industrial and scientific,
through the XLDB conference and workshop series
we lead, and beyond.

To understand query complexity and expected access patterns, we are
working with LSST Science Collaborations and the LSST Science Council to
understand the expected query load and query complexity. We have
compiled a set of common queries \citep{CommonQueries} and distilled
this set into a smaller set of representative queries we use for various
scalability tests--this set represents each major query type, ranging
from trivial low volume, to complex correlations \citep{PerfTests}. We
have also talked to scientists and database developers from other
astronomical surveys, including SDSS, 2MASS, Gaia, DES, LOFAR and
Pan-STARRS.

To deal with unpredictability of analysis, we will use shared scans.
With shared scans, users will have access to all the data, all the
columns, even these very infrequently used, at a predictable cost --
with shared scans increasing complexity does not increase the expensive
disk I/O needs, it only increases the CPU needs.

To keep query load under control, we will employ throttling to limit
individual query loads.

\section{Implementation of the Query Service (Qserv)
Prototype}\label{implementation}

To demonstrate feasibility of running LSST queries without relying on
expensive commercial solutions, and to mitigate risks of not having an
off-the-shelf system in time for LSST construction, we built a prototype
system for user query access, called \emph{Query Service} (Qserv). The
system relies on two production-quality components: MySQL and
XRootD. The prototype closely follows the LSST
baseline database architecture described in \secref{baseline-architecture}.

\subsection{Components}\label{components}

\subsubsection{MySQL}\label{mysql}

MySQL is used as an underlying SQL execution engine. To control the
scope of effort, Qserv uses an existing SQL engine, MySQL, to perform as
much query processing as possible. MySQL is a good choice because of its
active development community, mature implementation, wide client
software support, simple installation, lightweight execution, and low
data overhead. MySQL's large development and user community means that
expertise is relatively common, which could be important during Qserv's
development or long-term maintenance in the years ahead. MySQL's MyISAM
storage engine is also lightweight and well-understood, giving
predictable I/O access patterns without an advanced storage layout that
may demand more capacity, bandwidth, and IOPS from a tightly constrained
hardware budget.

It is worth noting, however, that Qserv's design and implementation do
not depend on specifics of MySQL beyond glue code facilitating results
transmission. Loose coupling is maintained in order to allow the system
to leverage a more advanced or more suitable database engine in the
future.

\subsubsection{XRootD}\label{xrootd}

The XRootD distributed file system is used to
provide a distributed, data-addressed, replicated, fault-tolerant
communication facility to Qserv. Re-implementing these features would
have been non-trivial, so we wanted to leverage an existing system.
XRootD has provided scalability,
fault-tolerance, performance, and efficiency for over 10 years of in the
high-energy physics community, and its relatively flexible API enabled
its use as a more general communication medium instead of a file system.
Since it was designed to serve large data sets, we were confident that
it could mediate not only query dispatch communication, but also bulk
transfer of results.

A XRootD cluster is implemented as a set of
data servers and a redirector(s). A client connects to the redirector,
which acts as a caching namespace lookup service that redirects clients
to appropriate data servers. In Qserv, XRootD
data servers become Qserv workers by plugging custom code into
XRootD as a custom file system implementation.
The Qserv master dispatches work as an XRootD
client to workers by writing to partition-addressed
XRootD paths and reads results from
hash-addressed XRootD paths.

\begin{figure}[H]
\centering
\includegraphics{_static/xrootd.png}
\caption{XRootD.}
\end{figure}

While the primary purpose of XRootD is to
provide a distributed clustered file system, it is implemented as a
plug-in component based system that allows ``file'' to be replaced by
any other resource object. Qserv makes use of this capability to cluster
MySQL databases instead of files. Hence, we dispense with calling
XRootD a distributed file system and simply
call is a generic system for providing named communication paths,
clustering, request scheduling, and error recovery.

\subsection{Partitioning}\label{partitioning-1}

In Qserv, large spatial tables are fragmented into spatial pieces in the
two-level partitioning scheme. The partitioning space is a spherical
space defined by two angles $\phi$ (right ascension/$\alpha$) and $\theta$ (declination/$\delta$).
For example, the Object table is fragmented spatially, using a
coordinate pair specified in two columns--right-ascension and
declination. On worker nodes, these fragments are represented as tables
named \emph{Object\_CC} and \emph{Object\_CC\_SS} where \emph{CC} is the
``chunk id'' (first-level fragment) and \emph{SS} is the ``sub-chunk
id'' (second-level fragment of the first larger fragment. Sub-chunk
tables are built on-the-fly to optimize performance of spatial join
queries. Large tables are partitioned on the same spatial boundaries
where possible to enable joining between them.

\subsection{Query Generation}\label{query-generation}

Qserv is unusual (though not unique) in processing a user query into one
or more queries that are subsequently executed on off-the-shelf
single-node RDBMS software. This is done in the hopes of providing a
distributed parallel query service while avoiding a full
re-implementation of common database features. However, we have found
that it is necessary to implement a query processing framework much like
one found in a more standard database, with the exception that the
resulting query plans contain SQL statements as the intermediate
language.

A significant amount of query analysis not unlike a database query
optimizer is required in order to generate a distributed execution plan
that accurately and efficiently executes user queries. Incoming user
queries are first parsed into an intermediate representation using a
modified SQL92-compliant grammar (Lubos Vnuk's SqlSQL2). The resulting
query representation is equivalent to the original user query, and does
not include any stateful interpretation, but may not completely reflect
the original syntax. The purpose of this representation is to provide a
semantic representation that may be operated upon by query analysis and
transformation modules without the complexity of a parse tree containing
every node in the original EBNF grammar.

Once the representation has been created, the query representation is
processed by two sequences of modules. The first sequence operates on
the query as a single statement. A transformation step occurs to split
the single representation into a ``plan'' involving multiple phases of
execution, one to be executed per-data-chunk, and a one to be executed
to combine the distributed results into final user results. The second
sequence is applied on this plan to apply the necessary transformations
for an accurate result.

We have found that regular expressions and parse element handlers to be
insufficient to analyze and manipulate queries for anything beyond the
most basic query syntax constructions.

\subsubsection{Processing modules}\label{processing-modules}

The processing modules perform most of the work in transforming the user
query into statements that can produce a faithful result from a Qserv
cluster. These include:

\begin{itemize}
\item
  Identify spatial indexing opportunities. This allows Qserv to dispatch
  spatially-restricted queries on only a subset of the available chunks
  constituting a table. Spatial restrictions given in Qserv-specific
  syntax are rewritten as boolean SQL clauses.
\item
  Identify secondary index opportunities. Qserv databases designate one
  column (more are under consideration) as a key column where its values
  are guaranteed to exist in one spatial location. Identification allows
  Qserv to convert point queries on this column into spatial
  restrictions.
\item
  Identify table joins and generate syntax to perform distributed join
  results. Qserv primarily supports ``near-neighbor'' spatial joins for
  limited distances defined in the partitioning coordinate space.
  Arbitrary joins between distributed tables are only supported using
  the key column. Classify queries according to data coverage and table
  scanning. By identifying tables scanned in a query, Qserv is able to
  mark queries for execution using shared scanning, which greatly
  increases efficiency.
\end{itemize}

\subsubsection{Processing module
overview}\label{processing-module-overview}

\begin{figure}[H]
\centering
\includegraphics{_static/processing_modules.png}
\caption{Processing modules.}
\end{figure}

This figure illustrates the query preparation pipeline that generates
physical queries from an input query string. User query strings are
parsed (1) into a structured query representation that is passed through
a sequence of processing modules (2) that operate on that representation
in-place. Then, it is broken up (3) into pieces that are explicitly
intended for parallel execution on table partitions and pieces intended
to merge parallel results into user results. Another processing sequence
(4) operates on this new representation, and then finally, concrete
query strings are generated (5) for execution.

The two sequences of processing modules provide an extensible means to
implement query analysis and manipulation. Earlier prototypes performed
analysis and manipulation during parsing, but this led to a practically
unmaintainable code base and the functionality has been ported the
processing module model. Processing is split into two sequences to
provide the flexibility to modules that manipulate the physical
structures while offering the simpler single-query representation to
modules that do not require the complexity. The clear separation between
parsing, whose only goal is to provide a intelligible and modifiable
query representation, and the qserv-specific analysis and manipulation
is a key factor in the overall flexibility, maintainability, and
extensibility of the system and should help the system adapt to current
and future LSST needs.

\subsection{Dispatch}\label{dispatch}

The baseline Qserv uses XRootD as a
distributed, highly-available communications system to allow Qserv
frontends to communicate with data workers. Up until 2015, Qserv used a
synchronous client API with named files as communication channels. The
current baseline system utilizes a general two-way named-channeling
system which eliminates explicit file abstractions in favor of
generalized protocol messages that can be flexibly streamed. The scheme
is called Scalable Service Interface (SSI) and is built on top
of XRootD. The interface was specifically
designed to hide underlying XRootD
dependencies. This allows switching the underlying implementation with
minimal impact to Qserv.

\subsubsection{Wire protocol}\label{wire-protocol}

Qserv encodes query dispatches in ProtoBuf messages, which contain SQL
statements to be executed by the worker and annotations that describe
query dependencies and characteristics. Transmitting query
characteristics allows Qserv workers to optimize query execution under
changing CPU and disk loads as well as memory considerations. The worker
need not re-analyze the query to discover these characteristics or guess
at conditions that cannot be determined by query inspection.

Query results are also returned by ProtoBuf messages. Current versions
transmit a MySQL dump file allowing the query results to be faithfully
reproduced on the Qserv frontend, but the baseline system will transmit
results directly. Initial implementations avoided logic to encode and
decode data values, but experience with the prototype MonetDB worker
backend proved that data encoding and marshalling were a contained
problems whose solution could significantly improve overall query
latency by avoiding mutating metadata operations on worker and frontend
DBMS systems. Thus the baseline system will encode results in protobuf
messages containing schema and row-by-row encoded data values. Streaming
results directly from worker dbms instances into frontend dbms instances
is a technique under consideration, as is a custom aggregation engine
for results that would likely ease the implementation of providing
partial query results to end users.

\subsubsection{Frontend}\label{frontend}

ABH In 2012, a new XRootD client API was
developed to address our concerns over the older version's scalability
(uncovered during a 150 node, 30TB scalability test \citedsp{DMTR-21}). The new client API
began production use for the broader XRootD
community in late 2012. Subsequently, work began under our guidance
towards an XRootD Qserv client API that was
based on request-response interaction over named channels, instead of
opening, reading, and writing files. A production version of this API,
the Scalable Service Interface (SSI) became available in early
2015 and Qserv has since been ported to use this interface. The port
eliminated a significant body of code that maps dispatching and
result-retrieving to file operations. The SSI API will reside
in the Xroot code base, where it may be exercised by other projects.

The SSI API provides Qserv with a fully asynchronous interface
that eliminates nearly all blocking threads used by the Qserv frontend
to communicate with its workers. This eliminated one class of problems
we have encountered during large-scale testing. The SSI API has
defined interfaces that integrate smoothly with the Protobufs-encoded
messages used by Qserv. Two novel features were specifically added to
improve Qserv performance. The streaming response interface enables
reduced buffering in transmitting query results from a worker mysqld to
a the frontend, which lowers end-to-end query latency and reduces
storage requirements on workers. The out-of-band meta-data response
which arrives prior to the data results can be used to map out the
Protobufs encoding and significantly simplify handling response memory
buffers.

The fully asynchronous API is crucial on the master because of the large
number of concurrent chunk queries in flight expected in normal
operation. For example, with the sky split into 10k pieces, having 10
full-scanning queries running concurrently would have 100k concurrent
chunk queries--too large a number of threads to allow on a single
machine. Hence, an asynchronous API to XRootD
is crucial. Threads are used to parallelize multiple CPU-bound tasks.
While it does not seem to be important to parse/analyze/manipulate a
single user query in parallel (and such a task would be a research
topic), the retrieval and processing of results could be done in
parallel if some portion of the aggregation/merging were done in Qserv
code rather than loaded into the frontend's MySQL instance and merged
via SQL queries. Thus results processing should be parallelized among
results from individual chunks, and query parsing/analysis/manipulation
can be parallelized among independent user queries.

\subsubsection{Worker}\label{worker}

The Qserv worker uses both threads and asynchronous calls to provide
concurrency and parallelism. To service incoming requests from the
XRootD API, an asynchronous API is used to
receive requests and enqueue them for action. Specifically, the Scalable
Service Interface (SSI) is used on Qserv workers as well. The
interface provides a mirror image of the actions taken on the front-end
making the logic relatively easy to follow and the implementation less
error prone.

Threads are maintained in a thread pool to perform incoming queries and
wait on calls into the DBMS's API (currently, the apparently synchrnous
MySQL C-API). Threads are allowed to run in observance of the amount of
parallel resources available. The worker estimates the I/O dependency of
each incoming chunk query in terms of the chunk tables involved and disk
resources involved, and attempts to ensure that disk access is almost
completely sequential. Thus if there are many queries that access the
same table chunk, the worker allows as many of them to run as there are
CPU cores in the system, but if it has many queries that involve
different chunk tables, it allows fewer simultaneous chunk queries in
order to ensure that only one table scan per disk spindle occurs.
Further discussion of this ``shared scanning'' feature is described in
\citeds{DMTN-048}.

\subsection{Threading Model}\label{threading-model}

Nearly every portion of Qserv is written using a combination of threaded
and asynchronous execution.

Qserv heavily relies on multi-threading to take advantage of all
available CPU cores when executing queries, as an example, to complete
one full table scan on a table consisting of 1,000 chunks, 1,000 queries
(processes) will be executed. To efficiently handle large number of
processes that are executed on each worker, we ended up rewriting the
XRootD client and switching from
thread-per-request model to a thread-pool model. The new client is
completely asynchronous, with real call-backs.

\begin{longtable}{|l|p{0.6\textwidth}|}
\hline
mysqlproxy & Single-threaded Lua code \\ \hline
Frontend-python & Single-threaded asynchronous reactor; blocking-thread per user
query \\ \hline
Frontend-C++ & Processing thread per user-query for preparation; Results-merging
thread-per-user-query on-demand; \\ \hline
Frontend-xrootd &
Callback threads perform query transmission and results retrieval \\ \hline
Frontend-xrootd internal &
Threads for maintaining worker connections (\textless{} 1 per
host) \\ \hline
Xrootd, cmsd &
Small thread pools for managing live network connections and performing
lookups \\ \hline
Worker-xrootd plugin &
Small thread pool O(\#cores) to make blocking mysql C-API calls into
local mysqld; callback threads from XRootD
perform admission/scheduling of tasks from frontend and transmission of
results \\ \hline
\hline
\end{longtable}

\subsection{Aggregation}\label{aggregation}

Qserv supports several SQL aggregation functions: AVG(), COUNT(), MAX(),
MIN(), and SUM(), and should support SQL92 level GROUP BY.

\subsection{Indexing}\label{indexing-1}

The secondary index utilizes one or more tables using the InnoDB storage
engine on each czar to perform lookups on the database's key colume
(objectId). Performance tests (Figure~\ref{fig:indexing-tests}) on a
single, dual-core host with 1 TB hard disk storage (not SSD) have shown
that this configuration will support a full load of 40 billion rows in
about 400,000 seconds (110 hours). A more realistic configuration with
multiple cores and SSD storage is expected to meet the requirement of
fully loading in less than 48 hours.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{_static/indexing_tests.png}
\caption{Performance tests of MySQL-based secondary index. \label{fig:indexing-tests}}
\end{figure}

To improve the performance of the InnoDB storage engine for queries, the
secondary index may be split across a small number (dozens) of tables,
each containing a contiguous range of keys. This splitting, if done,
will be independent of the partitioning of the database itself. The
contiguity of key ranges will allow the secondary index service to
identify the appropriate split table arithmetically via an in-memory
lookup.

\subsubsection{Secondary Index
Structure}\label{secondary-index-structure}

The secondary index consists of three columns: the key (objectId), the
chunk where all data with that key are located (chunkId), and the
subchunk within that chunk where data with the key are located
(subChunkId). The objectId is assigned by the science pipelines as a
64-bit integer value; the chunkId and subChunkId are both 16-bit
integers which identify spatial regions on the sky.

\subsubsection{Secondary Index Loading}\label{secondary-index-loading}

The InnoDB storage engine loads tables most efficiently if it is
provided input data which has been presorted according to the table's
primary key. When the secondary index information is collected for
loading (from each worker node handling a collection of chunks), it is
sorted by objectId, and may be divided into roughly equal ``splits''.
Each of those splits is loaded into a table \emph{en masse}.

To fully optimize the loading and table splitting, the entire index
should be collected from all workers and pre-sorted in memory on the
czar. This is not reasonable for 40 billion entries (requiring a minimum
of 480 GB memory, plus overhead). Instead, the index data from a single
worker can be assumed to be a ``representative sample'' from the full
range of objectIds, so table splitting can be done using the first
worker's index data. The remaining workers will be split and loaded
according to those defined tables.

\subsection{Data Distribution}\label{data-distribution}

LSST will maintain the released data store both on tape media and on a
database cluster. The tape archive is used for long-term archival. Three
copies of the compressed catalog data will be kept. The database cluster
will maintain 3 online copies of the data. Because computer clusters of
reasonable size failure regularly, the cluster must maintain replicas in
order to provide continuous data access. A replication factor of 3 (r=3)
is needed in order to determine data integrity by majority rule when one
replica is corrupt.

If periodic unplanned downtime is acceptable, an on-tape replica may
function as one of the three. However, the use of tape dramatically
increases the cost of recovering from a failure. This may be acceptable
for some tables, particularly those that are large and lesser-used,
although allowing service disruption may make it difficult to make
progress on long-running analysis on those large tables.

\subsubsection{Database data
distribution}\label{database-data-distribution}

The baseline database system will provide access for two database
releases: latest and previous . Data for each release will be spread out
among all nodes in the cluster.

Data releases are partitioned spatially, and spatial pieces (chunks) are
distributed in a round-robin fashion across all nodes. This means that
area queries involving multiple chunks are almost guaranteed to involve
resources on multiple nodes.

Each node should maintain at least 20\% free space of its data storage
allocation. The remaining free space is then available to be
``borrowed'' when another node fails. This will a temporary use of
storage capacity until more server resources can be put online, until
the 80\% storage use is returned.

\subsubsection{Failure and integrity
maintenance}\label{failure-and-integrity-maintenance}

There will be failures in any large cluster of node, in the nodes
themselves, in data storage volumes, in networks access and so on. These
failures will remove access to data that is resident on those nodes, but
this loss of data access should not affect that ability of scientists to
analyze the dataset as a whole. We need to set a data availability time
over 99.5\% to ensure confidence of the community in the stability of
the system. To ensure this level of data access, and to allow acceptable
levels of node failures in a cluster, there will be replication of data
on a table level throughout the cluster.

The replication level will be that each table in the database will exist
3 times, each on separate nodes. A monitoring layer to the system will
check on the availability of each table every few hours, although this
time will be tuned in practice. When this layer sees that a table has
less than three replicas available, this will initiate a replication of
that table to another nodes, not currently hosting that table. The times
for the checking, and speed of replication will be tuned to the
stability of the cluster, such that about 5\% of all tables at any given
time will only have 1 or 2 replicas. Three replicas will ensure that
tables will be available even in cases of large failures, or when nodes
need to be migrated to new hardware in bulk.

Should an entire node fail, replicating that data to another single node
would be fairly expensive in terms of time. As of July 2013, a 3TB drive
will have a write speed of 60/150/100 MB/s (min/max/avg) \citep{Kirsch:2012}
and refilling
this single drive would remove access to that replica of the data for
about 8 hours. We plan on having free space on each node, and only fill
local storage to 80\%. The free space will be used for temporary storage
of tables on failures, where replicas can take place in parallel between
nodes into this free space. When new nodes with free storage are added
to the cluster, then this data can be copied off this free space into
the drive, taking the full 8 hours, but there will still be 3 replicas
of data during this time. Once this is complete, this data will have 4
replicas for the short period of time while these tables can be removed
from the temporary storage, returning each node to 80\% usage.

\subsection{Metadata}\label{metadata}

Qserv needs to track various metadata information, static (not changing
or changing very infrequently), and dynamic (run-time) in order to
maintain schema and data integrity and optimize the cluster usage.

\subsubsection{Static metadata}\label{static-metadata}

Qserv typically works with databases and tables distributed across many
different machines: it breaks individual large tables into smaller
chunks and distributes them across many nodes. All chunks that belong to
the same logical table must have the same schema and partitioning
parameters. Different tables often need to be partitioned differently,
for example some tables might be partitioned with overlap (such as the
Object table), some might be partitioned with no overlap (for example
the Source table), and some might not need partitioning at all (e.g., a
tiny Filter table). Further, there might be different partitioning
strategies, such as spherical-box based, or HTM-based. All this
information about schema and partitioning for all Qserv-managed
databases and tables needs to be tracked and kept consistent across the
entire Qserv cluster.

Implementation of the static metadata in Qserv is based on hierarchical
key-value storage which uses a regular MySQL database as a storage
backend. This database is shared between multiple masters and it must be
served by a fault-taulerant MySQL server instance, e.g. using a
master-master replication solution like MariaDB Galera cluster. Database
consistency is critical for metadata and it should be implemented using
one of the transactional database engines in MySQL.

Static metadata may contain following information:

\begin{itemize}
\item
  Per-database and per-table partitioning and scan scheduling
  parameters.
\item
  Table schema for each table, used to create database tables in all
  worker and master instances; the schema in the master MySQL instance
  can be used to obtain the same information when a table is already
  created.
\item
  Database and table state information, used primarily by the process of
  database and table creation or deletion.
\item
  Definitions for the set of worker and master nodes in a cluster
  including their availability status.
\end{itemize}

The main clients of the static metadata are:

\begin{itemize}
\item
  Administration tools (command-line utilities and modules) which allow
  one to define or modify metadata structures.
\item
  Qserv master(s), mostly querying partitioning parameters but also
  allowed to modify table/database status when deleting/creating new
  tables and databases. Master(s) should not depend on node definitions
  in metadata, the xrootd facility is used to communicate with workers.
\item
  Special ``watcher'' service which implements distributed process of
  database and table management.
\item
  An initial implementation of the data loading application which will
  use the node definitions and will create/update database and table
  definitions. This initial implementation will eventually be replaced
  by a distributed loading mechanism which may be based on separate
  mechanisms.
\end{itemize}

\subsubsection{Dynamic metadata}\label{dynamic-metadata}

In addition to static metadata, a Qserv cluster also needs to track its
current state, and keep various statistics about query execution. This
sort of data is udated frequently, several times per query execution,
and is called dynamic metadata.

Prototype implementation of the dynamic metadata is based on MySQL
database. Like static metadata it needs to be shared between all master
instances and will be served via a single fault-talerant MySQL instance
which will be shared with static metadata database.

Dynamic metadata will contain the following information:

\begin{itemize}
\item
  Definition of every master instance in a Qserv cluster.
\item
  Record of every SELECT-type query processed by cluster. This record
  includes query processing state and some statistical/timing
  information.
\item
  Per-query list of table names used by the asynchronous queries, this
  information is used to delay table deletion while async queries are in
  progress.
\item
  Per-query worker information, which includes chunk id and identifying
  information for the worker processing that chunk id. This information
  will be used to transparently restart the master or migrate query
  processing to a different master in case of master failure.
\end{itemize}

The most significant use of the dynamic metadata is to track execution
of asyncronous queries. When an async query is submitted it is
registered in dynamic metadata and its ID is returned to the user
immediately. Later users can request status information for that query
ID which is obtained from dynamic metadata. When query processing is
finished users can request results from that query, and the master can
obtain the location of the result data from dynamic metadata.

Additionally dynamic metadata can be used to collect statistical
information about queries that were executed in the past which may be an
important tool in understanding and improving system performance.

\subsubsection{Architecture}\label{architecture}

The Qserv metadata system is implemented based on master/server
architecture: the metadata is centrally managed by a \emph{Qserv
Metadata Server} (qms). The information kept on each worker is kept to a
bare minimum: each worker only knows which databases it is supposed to
handle, all remaining information can be fetched from the qms (through
Qserv) as needed. This follows our philosophy of keeping the workers as
simple as possible.

The real-time metadata is managed inside qms in in-memory tables,
periodically synchronized with disk-based table. Such configuration
allows reducing qms latency---important to avoid delaying query
execution time. Should a qms failure occur, the in-flight queries for
which the information was lost will be restarted. Since the
synchronization to disk-based table will occur relatively frequently
(eg. at least 1 per minute), the lost time is insignificant. To avoid
overloading the qms with, only the high-level information available from
Qserv-master is stored in qms; all worker-based information is cached in
a scratch space locally to each worker in a simple, raw form (e.g,
key-value, ASCII file), and can be fetched on demand as needed.

At the moment we use xml-rpc as a message protocol to communicate with
qms. It was a natural choice given that this protocol is already in use
by Qserv master.

\subsubsection{Typical Data Flow}\label{typical-data-flow}

Static metadata:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Parts of the static metadata known before data is partitioned/loaded
  are loaded by the administration scripts responsible for loading data
  into the database, then these scripts start data partitioner.
\item
  The data partitioner reads static metadata loaded by the
  administration scripts, loads remaining information.
\item
  When Qserv starts, it fetches all static metadata and caches it in
  memory in a special, in-memory optimized C++ structure.
\item
  The contents of the in-memory metadata cache inside Qserv can be
  refreshed on demand if the static metadata changes (for example, when
  a new database or a table is added).
\end{enumerate}

Dynamic-metadata:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Master loads the information for each query (when it starts, when it
  completes).
\item
  Detailed statistics are dumped by each worker into a scratch space
  kept locally. This information can be requested from each worker on
  demand. A typical use case: if all chunk-queries except one completed,
  qms would fetch statistics for the still-running chunk-query to
  estimate when the query might finish, whether to restart this query
  etc.
\end{enumerate}

\subsection{Shared Scans}\label{shared-scans}

Arbitrary full-table scanning queries must be supported in LSST's
baseline catalog, and in order to provide this support cost-effectively
and efficiently, Qserv implements shared scans. Shared scans effectively
reduces the I/O cost of executing multiple scanning queries
concurrently, reducing the system hardware need and purchasing costs.

Shared scans reduce overall I/O costs by forcing incoming queries to
share. When multiple queries scan the same table, theoretically, they
can completely share I/O and incur only the I/O cost of a single query
rather than the sum of their individual costs. In general, it is
difficult for queries to share I/O because their arrival times are
random and uncorrelated. Each query begins scanning at different times,
and because LSST's catalog tables will be so large, general system
caching is ineffective. In Qserv, scanning queries are broken up into
many parts, and shared scanning forces each query to operate on the same
portion and thus share I/O cost, rather than allowing each to perform
its own ordered scan and incur costs individually.

\subsubsection{Background}\label{shared-scan-background}

Historically, shared scanning has been a research topic that has very
few real-world implementations. We know of only one implementation in
use (Teradata). Most database implementations assume OS or database
caching is sufficient, encouraging heavy use of indexing to reduce the
need of table scans. However, our experiments have shown that when
tables are large enough (by row count) and column access sufficiently
variable (cannot index enough columns when there are hundreds to choose
from), indexes are insufficient. With large tables, indexes no longer
fit in memory, and even when they do fit in memory, the seek cost to
retrieve each row is dominant when the index selects a percentage of
rows, rather than some finite number (thousands or less).

\subsubsection{Implementation}\label{shared-scan-implementation}

The implementation of shared scans in Qserv is in two parts. The first
part is a basic classification of incoming queries as scanning queries
or non-scanning queries. A query is considered to scan a table if it
depends on non-indexed column values and involves more than \emph{k}
chunks (where \emph{k} is a tunable constant). Note that involving
multiple chunks implies that the query selects from at least one
partitioned table. This classification is performed during query
analysis on the front-end and leveraging table metadata. The metadata
includes a ``scan rating'', which is set by hand. Higher scan ratings
indicate larger tables that take longer to read from disk. The
identified ``scan tables'' and their ratings are marked and passed along
to Qserv workers, which use the information in scheduling the fragments
of these scanning queries.

The second part of the shared scans implementation is a scheduling
algorithm that orders query fragment execution to optimize cache
effectiveness. Because Qserv relies on individual off-the-shelf DBMS
instances on worker nodes, it is not allowed to modify those instances
to implement shared scans. Instead, it issues query fragments ordered to
maximize locality of access in data and time, and tries to lock the
files associated with the tables in memory as much as possible. Using
the identified scan tables and their ratings, the worker places them on
the appropriate scheduler. There will be at least three schedulers. One
for queries expected to complete in under an hour, which are expected to
be related to the Object table. One for queries expected to take less
than eight hours, expected to be related to Object\_Extra. And one for
scans expected to take eight to twelve hours for ForcedSource and/or
Source tables. The reasoning being that a single slow query can impede
the progress of a shared scan and all the other user queries on that
scan. There may be a need for another scheduler to handle queries taking
more than 12 hours.

Each scheduler places incoming chunk queries into one of two priority
queues sorted by chunk id then scan rating of the individual tables. If
the query is for a chunk after the currently scanning chunk id, it is
placed on the active priority queue, otherwise it is placed on the
pending priority queue. After chunk id, the priority queue is sorted by
the table with highest scan rating to ensure that the largest tables in
the chunk are grouped together.

Once the query is on the appropriate scheduler, the algorithm proceeds
as follows. When a dispatch slot is available, it checks the highest
priority scheduler. If that scheduler has a query fragment, hereafter
called tasks, and it is not at its quota limit, it is allowed to start
its next task, otherwise the worker checks the next scheduler. It
continues doing this until a task has been started or all the schedulers
have been checked.

Each scheduler is only allowed to start a task under certain
circumstances. There must be enough threads available from the pool so
that none of the other schedulers are starved for threads as well as
enough memory available to lock all the tables for the task in memory.
If the scheduler has no tasks running, it may start one task and have
memory reserved for the tables in that task. This should prevent any
scheduler from hanging due to memory starvation without requiring
complicated logic but could incur extra disk I/O. More on locking tables
in memory later.

Schedulers check for tasks by first checking the top of the active
priority queue. If the active priority queue is empty, and the pending
priority queue is not, then the active and pending queues are swapped
with the task being taken from the top of the ``new'' active queue.

Since the queries are being run by a separate DBMS instance of which
there is little control of how it goes about running queries, the worker
can control when queries are sent to the DBMS and also lock files in
memory. Files in memory are among the most likely items to be paged out
when memory resources are low, which would increase disk I/O. Locking
files in memory prevents this from happening. However, care must taken
in choosing how much memory can be used for locking files. Use too much
and there will be a significant impact on DBMS performance. Set aside
too little, and schedulers will not make optimum use of the resources
available and may be forced to run tasks without actually locking the
files in memory.

The memory manager controls which files are locked in memory. When a
scheduler tries to run a task, the task asks the memory manager to lock
all the shared scan tables it needs. The memory manager determines which
files are associated with the tables. If the files are already locked in
memory and there is enough memory available to lock the files which are
not already locked, the task is given a handle and allowed to run. When
the task completes, it hands the handle back to the memory manager. If
it was the last task using any particular table, the memory for the
files used by that table is freed.

When the memory manager locks a file, it does not read the file. It only
sets aside memory for the file to occupy when it is read by the DBMS. In
the special case where a task can run even though there is not enough
memory available, those tables that cannot fit are put on a list of
reserved tables and their size is subtracted from the quota until they
can be locked or freed. When memory is freed, the memory manager will
try to lock the reserved tables.

Because Qserv processes interactive, short queries concurrently with
scanning queries, its query scheduler should be able to allow for those
queries to complete without waiting for a query scan. To achieve this,
Qserv worker nodes choose between the scan scheduler described above and
a simpler \emph{grouping} scheduler. Incoming queries with identified
scan tables are admitted to the scan scheduler, and all other queries
are admitted to the grouping scheduler. The grouping scheduler is a
simple scheduler that is a simple variant of a plain FIFO
(first-in-first-out) scheduler. Like a FIFO scheduler, it maintains a
queue of queries to execute, and operates identically to a FIFO
scheduler with one exception--queries are grouped by chunk id. Each
incoming query is inserted into the queue behind another query on the
same chunk, and at the back if no queued query matches. The grouping
scheduler assumes that the queue will never get very long, because it is
intended to only handle short interactive queries lasting fractions of
seconds, but groups its queue according to chunk id in order to provide
a minimal amount of access locality to improve throughput at a limited
cost to latency. Some longer queries will be admitted to the grouping
scheduler even though they are scanning queries, provided that they have
been determined to only scan a single chunk. Although these non-shared
scan query will disrupt performance of the overall scan on the
particular disk on a worker, the impact is thought to be small because
each of these represents all (or a large fraction of) the work for a
single user query, and the impact is amortized among all disks on all
workers.

For discussion about the performance of the existing prototype, refer to
demo-shared-scans.

\subsubsection{Memory management}\label{shared-scan-memory-management}

To minimize system paging when multiple threads are scanning the same
table, we implemented a memory manager called memman. When a
shared scan is about to commence, the shared scan scheduler informs
memman about the tables the query will be using and how
important it is to keep those tables in memory during the course of the
query. When directed to keep the tables in memory, memman opens
each data base table file, maps it into memory, and then locks the pages
to prevent the kernel from stealing the pages for other uses. Thus, once
a file page is faulted in, it stays in memory and allows other threads
to scan the contents of the page without incurring additional page
faults. Once the shared scan of the table completes, memman is
told that the tables no longer need to remain in memory. memman
frees up the pages by unlocking them and deleting the mapping.

This type of management is necessary to satisfy system paging
requirements because the prime paging pool is the set of unlocked file
system pages.

\subsubsection{\texorpdfstring{XRootD
scheduling
support}{XRootD scheduling support}}\label{shared-scan-xrootd-scheduling-support}

When the front-end dispatches a query, the
XRootD normally picks the least used server in
an attempt to spread the load across all of the nodes holding the
required table. While this works well for interactive queries, it is
hardly ideal for shared scan queries. In order to optimize memory and
I/O usage, queries for the same table in a shared scan should all be
targeted to the same node. A new scheduling mode was added to the
XRootD cmsd called affinity scheduling. The
front-end can tell XRootD whether or not a
particular query has affinity to other queries using the same table.
Queries that have affinity are always sent to the same node relative to
the table they will be using. This allows the back-end scheduler to
minimize paging by running the maximum number of queries against the
same table in parallel. Should that node fail,
XRootD assigns another working node that has
the table as the target node for queries that have affinity.

\subsubsection{Multiple tables support}\label{shared-scan-multiple-tables-support}

Handling multiple tables in shared scans requires an additional level of
management. The scheduler will aim to satisfy a throughput yielding
average scan latencies as follows:

\begin{itemize}
\item
  \texttt{Object} queries: 1 hour
\item
  \texttt{Object}, \texttt{Source} queries (join): 12 hours
\item
  \texttt{Object}, \texttt{ForcedSource} queries (join): 12 hours
\item
  \texttt{Object\_Extras}\footnote{This includes all
    \texttt{Object}-related tables, e.g., \texttt{Object\_Extra},
    \texttt{Object\_Periodic}, \texttt{Object\_NonPeriodic},
    \texttt{Object\_APMean}} queries (join): 8 hours.
\end{itemize}

As stated in 8.10.2, there will be schedulers for queries that are
expected to take one hour, eight hours, or twelve hours. The schedulers
group the the tasks by chunk id and then the highest scan rating of the
all tables in the task. The scan ratings are meant to be unique per
table and indicative of the size of the table, so that this sorting
places scans using the largest table from the same chunk next to each
other in the queue. Using scan rating allows flexibility to work with
data sets with schemas different than that of LSST.

Since scans are not limited to specific tables, complicated joins could
occur in user queries that could take more than twelve hours to process.
The worker may also need to be able to identify user queries that are
too slow for the current scheduler based on the time it takes to
complete tasks for that query. This indicates there may be a need for a
scheduler to handle queries with very long run times.

\subsection{Level 3: User Tables, External
Data}\label{level-3-user-tables-external-data}

Level 3 tables including tables generated by users, and data catalogs
brought from outside, depending on their type and size, will be either
partitioned and distributed across the production database servers, or
kept unpartitioned in one central location. While the partitioned and
distributed Level 3 data will share the nodes with Level 2 data, it will
be kept on dedicated disks, independent from the disks serving Level 2
data. This will simplify maintenance and recoverability from failures.

Level 3 tables will be tracked and managed through the Qserv Metadata
System (qms), described in \secref{metadata}. This
includes both the static, as well as the dynamic metadata.

\subsection{Cluster and Task
Management}\label{cluster-and-task-management}

Qserv delegates management of cluster nodes to
XRootD. The XRootD
system manages cluster membership, node registration/de-registration,
address lookup, replication, and communication. Its Scalable Service
Interface (SSI) API provides data-addressed communication
channels to the rest of Qserv, hiding details like node count, the
mapping of data to nodes, the existence of replicas, and node failure.
The Qserv manager focuses on dispatching queries to endpoints and Qserv
workers focus on receiving and executing queries on their local data.

Cluster management performed outside of XRootD
does not directly affect query execution, but include coordinating data
distribution, loading, nodes joining/leaving and is discussed in
qserve-admin. The SSI API includes methods that allow dynamic
updates to the data view of an XRootD cluster.
So that when new tables appear or disappear, the
XRootD system will incorporate that
information for future scheduling decisions. Thus, clusters can
dynamically change without the need to restart the
XRootD system.

\subsection{Fault Tolerance}\label{fault-tolerance}

Qserv approaches fault tolerance in several ways. The design exploits
the immutability of the underlying data by replicating and distributing
data chunks across a cluster such that in the event of a node failure,
the problem can be isolated and all subsequent queries re-routed to
nodes maintaining duplicate data. Moreover, this architecture is
fundamental to Qserv's incremental scalability and parallel performance.
Within individual nodes, Qserv is highly modularized with minimal
interdependence among its components, which are connected via narrow
interfaces. Finally, individual components contain specialized logic for
minimizing, handling, and recovering from errors.

The components that comprise Qserv include features that independently
provide failure-prevention and failure-recovery capabilities. The MySQL
proxy is designed to balance its load among several underlying MySQL
servers and provide automatic fail-over in the event a server fails. The
XRootD system provides multiple managers and
highly redundant servers to provide high bandwidth, contend with high
request rates, and cope with unreliable hardware. And the Qserv master
itself contains logic that works in conjunction with
XRootD to isolate and recover from
worker-level failures.

A worker-level failure denotes any failure mode that can be confined to
one or more worker nodes. In principle, all such failures are
recoverable given the problem nodes are identified and alternative nodes
containing duplicate data are available. Examples of such failures
include a disk failure, a worker process or machine crashing, or network
problems that render a worker unreachable.

Consider the event of a disk failure. Qserv's worker logic is not
equipped to manage such a failure on localized regions of disk and would
behave as if a software fault had occurred. The worker process would
therefore crash and all chunk queries belonging to that worker would be
lost. The in-flight queries on its local mysqld would be cleaned up and
have resources freed. The Qserv master's requests to retrieve these
chunk queries via XRootD would then return an
error code. The master responds by re-initializing the chunk queries and
re-submits them to XRootD. Ideally, duplicate
data associated with the chunk queries exists on other nodes. In this
case, XRootD silently re-routes the request(s)
to the surviving node(s) and all associated queries are completed as
usual. In the event that duplicate data does not exist for one or more
chunk queries, XRootD would again return an
error code. The master will re-initialize and re-submit a chunk query a
fixed number of times (determined by a parameter within Qserv) before
giving up, logging information about the failure, and returning an error
message to the user in response to the associated query.

Error handling in the event that an arbitrary hardware or software bug
(perhaps within the Qserv worker itself) causes a worker process or
machine to crash proceeds in the same manner described above. The same
is true in the event that network loss or transient
sluggishness/overload has the limited effect of preventing
XRootD from communicating with one or more
worker nodes. As long as such failures are limited to a finite number of
workers and do not extend to the Qserv master node,
XRootD is designed to record the failure and
return an error code. Moreover, if duplicate data exists on other nodes,
this will be registered within XRootD, which
will successfully route any subsequent chunk queries.

In the event of an unrecoverable error, the Qserv master is equipped
with a status/error messaging mechanism designed to both log detailed
information about the failure and to return a human-readable error
message to the user. This mechanism includes C++ exception handling
logic that encapsulates all of the master's interactions with
XRootD. If an unrecoverable exception occurs,
the master gracefully terminates the query, frees associated resources,
logs the event, and notifies the user. Qserv's internal status/error
messaging system also generates a status message and timestamp each time
an individual chunk query achieves a milestone. Such milestones include:
chunk query dispatch, written to XRootD,
results read from XRootD, results merged, and
query finalized. This real-time status information provides useful
context in the event of an unrecoverable error.

Building upon the existing fault-tolerance and error handling features
described above, future work includes introducing a heart-beat mechanism
on worker nodes that periodically pings the worker process and will
restart it in the event it becomes unresponsive. Similarly, a master
monitoring process could periodically ping worker nodes and restart a
worker machine if necessary. We are also considering managing failure at
a per-disk level, but this would require research since
application-level treatment of disk failure is relatively rare. It
should also be possible to develop an interface for checking the
real-time status of queries currently being processed by Qserv by
leveraging its internally used status/error messaging mechanism.

\subsection{Next-to-database
Processing}\label{next-to-database-processing}

We expect some data analyses will be very difficult, or even impossible
to express through SQL language. This might be particularly useful for
time-series analysis. For this type of analyses, we will allow users to
execute their analysis algorithms in a procedural language, such as
Python. To do that, we will allow users to run their own code on their
own hardware resources co-located with production database servers.
Users then run queries on the production database which stream rows
directly from database cluster nodes to the user processing cluster,
where arbitrary code may run without endangering the production
database. This allows their incurred database I/O needs to be satisfied
using the database system's shared scanning infrastructure while
providing the full flexibility of running arbitrary code.

\subsection{Administration}\label{administration}

\subsubsection{Installation}\label{installation}

Qserv as a service requires a number of components that all need to be
running, and configured together. On the master node we require mysqld,
mysql-proxy, XRootD, cmsd, qserv metadata
service, and the qserv master process. On each of the worker nodes there
will also be the mysqld, cmsd, and XRootD
service. These major components come from the MySQL,
XRootD, and Qserv distributions. But to get
these to work together we will also require many more software package,
such as protobuf, lua, expat, libevent, python, zope, boost, java,
antlr, and so on. And many of these require more recent versions than
you are provided in most system distributions. We have an installation
layer, developed by SLAC, and LPC in Clermont-Ferrand, France in
collaboration, which will determine the packages, configure, compile and
install them in an automated process.

Currently, the Qserv installation procedure supports only the official
LSST platform--- RHEL6, and SL6 Linux distributions. Other UNIX-like
systems will be supported in the future as needed. The Qserv package
first can be downloaded from SLAC for install. In the initial README
there are basic install procedures, which start with a bootstrap script,
that will perform a yum install of needed packages distributed with
RHEL6, where the versions will support the Qserv install. Once that is
done an install script can be started. This will first download needed
packages not shipped with RHEL6 from SLAC, and get those installed
first. All software will be installed into a sandbox root path, and all
installed by the production username. Along with this is an install of
MySQL from source that will be configured for Qserv. These further
packages will be configured to run together, and then Qserv will be
complied and linked to these installed packages. All this runs without
user interaction, and usually completes within 15 to 20 minutes, to
provide a complete Qserv either master or worker node.

\subsubsection{Data loading}\label{data-loading}

As previously mentioned, Data Release Production will not write directly
to the database. Instead, the DRP pipelines will produce binary FITS
tables and image files that are reliably archived as they are produced.
Data will be loaded into Qserv in bulk for every table, so that tables
are either not available, or complete and immutable from the user query
access perspective.

For replicated tables, these FITS files are converted to CSV (e.g. by
harvesting FITS image header keyword value pairs, or by translating
binary tables to ASCII), and the resulting CSV files are loaded directly
into MySQL and indexed. For partitioned tables like Object and Source,
FITS tables are fed to the Qserv partitioner, which assigns partitions
based on sky coordinates and converts to CSV.

In particular, the partitioner divides the celestial sphere into
latitude angle ``stripes'' of fixed height H. For each stripe, a width W
is computed such that any two points in the stripe with longitudes
separated by at least W have angular separation of at least H. The
stripe is then broken into an integral number of chunks of width at
least W, so that each stripe contains a varying number of chunks (e.g.
polar stripes will contain just a single chunk). Chunk area varies by a
factor of about pi over the sphere. The same procedure is used to obtain
subchunks: each stripe is broken into a configurable number of
equal-height ``substripes'', and each substripe is broken into
equal-width subchunks. This scheme is preferred over the Hierarchical
Triangular Mesh for its speed (no trigonometry is required to locate the
partition of a point given in spherical coordinates), simplicity of
implementation, and the relatively fine control it offers over the area
of chunks and sub-chunks.

The boundaries of subchunks constructed as described are boxes in
longitude and latitude - the overlap region for a subchunk is defined as
the spherical box containing all points outside the subchunk but within
the overlap radius of its boundary.

The task of the partitioner is to find the IDs of the chunk and subchunk
containing the partitioning position of each row, and to store each row
in the output CSV file corresponding to its chunk. If the partitioning
parameters include overlap, then the row's partitioning position might
additionally fall inside the overlap regions of one or more subchunks.
In this case, a copy of the row is stored for each such subchunk (in
overlap CSV files).

Tables that are partitioned in Qserv must be partitioned identically
within a Qserv database. This means that chunk tables in a database
share identical partition boundaries and identical mappings of chunk id
to spatial partition. In order to facilitate table joining, a single
table's columns are chosen to define the partitioning space and all
partitioned tables (within a related set of tables) are either
partitioned according that pair of columns, or not partitioned at all.
Our current plan chooses the Object table's \texttt{ra\_PS} and
\texttt{decl\_PS} columns, meaning that rows in the Source and
ForcedSource tables will be partitioned according to the Objects they
reference.

There is one exception: we allow for pre-computed spatial match tables.
As an example, such a table might provide a many-to-many relationship
between the LSST Object catalog and a reference catalog from another
survey, listing all pairs of LSST Objects and reference objects
separated by less than some fixed angle. The reference catalog cannot be
partitioned by associated Object, as more than one Object might be
matched to a reference object. Instead, the reference catalog must be
partitioned by reference object position. This means that a row in the
match table might refer to an Object and reference object assigned to
different chunks stored on different Qserv worker nodes.

We avoid this complication by again exploiting overlap. We mandate (and
verify at partitioning time) that no match pair is separated by more
than the overlap radius. When partitioning match tables, we store a copy
of each match in the chunk of both positions referenced by that match.
When joining Objects to reference objects via the match table then, we
are guaranteed to find all matches to Objects in chunk C by joining with
all match records in C and all reference objects in C or in the overlap
region of C.

All Qserv worker nodes will partition subsets of the pipeline output
files in parallel -- we expect partitioning to achieve similar aggregate
I/O rates to those of full table scans for user query access, so that
partitioning should complete in a low factor (2-3x) of the table scan
time. Once it does, each Qserv worker will gather all output CSV files
for its chunks and load them into MySQL. The structure of the resulting
chunk tables is then optimized to maximize performance of user query
access (chunk tables will likely be sorted, and will certainly be
compressed), and appropriate indexes are built. Since chunks are sized
to fit in memory, all of these steps can be performed using an in-memory
file-system. I/O costs are incurred only when reading the CSV files
during the load and when copying finalized tables (i.e. .MYD/.MYI files)
to local disk.

The last phase of data loading is to replicate each chunk to one other
Qserv worker node. We will rely on table checksum verification rather
than a majority rule to determine whether a replica is corrupt or not.

The partitioner has been prototyped as a multi-threaded C++ program. It
uses an in-memory map-reduce implementation internally to scale across
cores, and can read blocks of one or more input CSV files in parallel.
It does not currently understand FITS table files. CSV file writes are
also parallelized - each output chunk is processed by a single reducer
thread and can be written to in parallel with no application level
locking. In preliminary testing, our partitioner was able to sustain
several hundred MB/s of both read and write bandwidth when processing a
CSV dump of the PT1.2 Source table.

We are investigating a pair of data loading optimizations. One is to
have pipeline processes either integrate the partitioning code or feed
data directly to the partitioner, rather than communicating via
persistent storage. The other is to write out tables in the native
database format (e.g. as .MYD files, ideally using the MySQL/MariaDB
server code to do so), allowing the CSV database loading step to be
bypassed.

\subsubsection{Administrative scripts}\label{administrative-scripts}

The administration of the qserv cluster will require a set of scripts,
all run from the one master machine, to control the large set of
workers. The main admin script, qserv-admin, will supply the base needs,
with starting all processes needed for the service, in order, and taking
down all processes to stop the service. Also base monitoring of service
is supplied here, to report on processes that are running, and
responding to base queries, to check on MySQL or
XRootD dying or locking up. Also is supplied
is the updating of the configuration definitions from the master out to
all workers, such that all machines need to have the same configurations
for the services.

The base data loading onto the nodes tends to be a slightly detailed
process, beyond the just the data preparation. Up to now, data
preparation produces text files in csv format, and then these will be
loaded into the MySQL layer as a MyISAM table. The schema for these
tables will need to have added to them the fields for chunk and subchunk
number needed for the Qserv service. The modification of the schema and
the control of the loading of the data, which can take hours, is done
with the qserv-load script. The loading of the data is also done without
index creation, and then that is done after the data loading. We are
also experimenting with the use of compressed read-only tables for the
data serving, and this is an option.

Another needed setup for the data service in qserv, is the creation of
the ``emptyChunks'' list. The data will be spatially partitioned into
``chunks'', as previously described, but the for the complete service,
the master process with need to know how many chunks exist in the data,
and which of these chunks contain no data. In queries which will involve
a complete table scan, which chunks to create query, or not, will need
to be known. Once the data is loaded, there is a another script which
will go out to nodes and see what chunks are there, and compile a list
of all possible chunks and which chunks do not contain data, or the
``emptyChunks'' lists. This is loaded by the qserv master process at
startup.

\subsection{Result Correctness}\label{result-correctness}

To verify Qserv does not introduce any unexpectedly alter results (e.g.,
does not show the same object twice or does not miss any objects on the
chunk boundaries), we developed an automated testbed, which allows us to
run pre-set queries on pre-set data sets both through plain MySQL and
through Qserv, and compare results.

\subsection{Current Status and Future
Plans}\label{current-status-and-future-plans}

As of now (June 2013) we have implemented a basic version of the system
end-to-end. Our prototype is capable of parsing a wide range of queries,
including queries executed by our QA system, ``PipeQA'', rewriting them
into sub-queries, executing these sub-queries in parallel and returning
results to the user. The implementation includes a generic parser, basic
query scheduler, job executor, query result collector. We demonstrated
running all query types (low, high, super-high such as large-area
near-neighbor) including aggregations, scalably on a 150-node cluster
using 30 TB data set \citedsp{DMTR-21}; and a smaller subset of queries scalably on
300-node cluster \citedsp{DMTR-12} (remaining tests in progress, expecting to complete in
the next 2-3 weeks) we also demonstrated the system performs well enough
to meet the LSST query response time requirements. We demonstrated the
system can handle high-level of concurrency (10 concurrent queries
simultaneously accessing 10,000 chunks each). We demonstrated the system
can recover from a variety of faults, or at minimum gracefully fail if
the error is unrecoverable. We extended SQL syntax coverage and ensured
the system is capable of supporting all types of queries executed over
the course of recent data challenges by PipeQA and users. We implemented
a core foundation for the metadata, currently used for managing static
metadata about Qserv-managed databases and tables, a set of
administrative tools, and scalable data partitioner. We made the system
easy to set up, resilient to typical failures and common user mistakes.
We implemented automated test bed. We consider the current prototype to
have a quality of a typical late-alpha / early-beta software.

Future work includes:

\begin{itemize}
\item
  extending metadata to support run-time statistics, implementing query
  management tools
\item
  implementing support for Level 3 data
\item
  completing initial shared scan implementation, testing and
  implementing concurrent and synchronized shared scans on multiple
  spindles
\item
  demonstrating cross-match with external catalogs
\item
  improving interfaces for users (eg hiding internal tables)
\item
  re-examining and improving query coverage, including more advanced SQL
  syntax, such as sub-queries as needed
\item
  improvements to administration scripts
\item
  support for HTM partitioning in Qserv
\item
  authentication and authorization
\item
  resource management
\item
  early partition results
\item
  performance improvements
\item
  partition granularity varying per table
\item
  security
\end{itemize}

\textbf{Extending metadata to support run-time statistics, implementing
query management tools}. Qserv currently does not maintain any explicit
run-time system state. Keeping such state would simplify managing Qserv
cluster, and building features such as query management: currently there
are no tools for inspecting and managing queries in-flight, and there
are no interfaces for halting queries except upon error detection. It is
clear that users and administrators will need to list running queries,
check query status and possibly abort queries.

\textbf{Implementing support for Level 3 data}. Qserv will need to
support level 3. That means users should be able to maintain their own
tables to store their own data or results from previous queries. They
should be able to create, drop, and update their own tables within the
system.

\textbf{Completing initial shared scan implementation, testing and
implementing concurrent and synchronized shared scans on multiple
spindles.} The first prototype implementation of shared scanning is
mostly complete, with the remaining work focused on basic analysis and
characterization of incoming user queries to determine scanning tables
and plumbing to convey the appropriate hints to worker nodes.

\textbf{Demonstrating cross-match with external catalogs}. One of the
use cases involves cross matching with external catalogs. In case the
catalogs to cross-match with is small, it will be treated as a small
table and replicated as metadata tables will be. For cross-matching with
larger catalogs, the catalog to cross-match with will need to be
partitioned and distributed on the worker nodes.

\textbf{Improving interfaces for users}. Many admin-type commands such
as ``list processes'' or ``explain'' are not ported to the distributed
Qserv architecture, and thus will not show correct result. At the moment
we have disabled these commands. Additionally, commands such as listing
tables in a given database will have to be overloaded, for example, we
should show user a table ``Object'' (even though in practice such table
does not exist in the Qserv system), instead of all the chunk
\texttt{Object\_XXX} tables, that are internal, and should not be
exposed to the end-user.

\textbf{Re-examining and improving query coverage, including more
advanced SQL syntax, such as sub-queries as needed}. We examined what
queries users and production processes execute, however we realize this
query set is far from the complete list of queries we will see in the
future. All needed syntax needs to be understood and fully supported.
Design and feasibility evaluation for sub-query support. Qserv does not
support SQL sub-queries. Since there is evidence that such a capability
might be useful to users, so we should formulate a few possible designs
and understand how easy/difficult they would be to implement. Note that
there are some alternative viable alternatives, such as splitting
sub-queries into multiple queries, and/or using session variables. A
naïve implementation that involves dumping all sub-query results to disk
and then reading these results from disk, similarly to how multiple
map/reduce stages are implemented, should be tractable to implement.

\textbf{Improvements to administration scripts}. To further automate
common tasks related to database management, table management, partition
management, data distribution, and others we need to implement many
improvements to the administration scripts.

\textbf{Support for HTM partitioning in Qserv}. HTM is an alternative to
the rectangular box form of spatial partitioning currently implemented
in Qserv. Since HTM allows for more advanced indexing and optimization,
it may eventually replace the current partitioning algorithm.

\textbf{Authentication and authorization}. The current Qserv does not
implement any form of security or privileges. All access is full access.
A production database system should provide some facility of user or
role-based access so that usage can be controlled and resources can be
shared. This is in particular needed for Level-3 data products.

\textbf{Resource management}. A production system should have some way
to manage/restrict resource usage and provide quality-of-service
controls. This includes a monitoring facility that can track each node's
load and per-user-query resource usage.

\textbf{Early partition results}. When performing interactive
exploration of an observational data set, users frequently issue
large-scale queries that produce undesired results, even after testing
such queries on small subsets of the data. We can ameliorate this
behavior by providing the investigator with early partial results from
the query, allowing the user to recognize that the returned values are
incorrect and permitting the query to be aborted without wasting
additional resources. There are two mechanisms we will implement in
Qserv for providing early results. First, for queries that retrieve a
filtered set of rows, matching rows can be returned as their query
fragments complete, well before all fragments finish. Second, for
queries that group, sort, or aggregate information and therefore perform
a global operation after any per-partition processing, the global
operation can be applied to increasingly large subsets of the
per-partition results, returning an early partial result each time.

\textbf{Performance improvements}. Significant performance gain can be
obtained by improving scheduler. These improvements pose interesting
state of the art computing challenges; more details are available in
Appendix D. In addition, some parts of Qserv are inefficient since they
were implemented under constraints of development time rather than
efficiency, or maintainability -- rewriting them would result in further
performance gains. Caching results for future queries is another example
of performance optimization that can yield significant speed
improvements.

\textbf{Partitioning granularity varying per table}. Since large tables
in LSST vary significantly in row count and row size, it may be
worthwhile to support partitioning with multiple granularities. For
execution management it is useful to have partitions sized so that query
fragments have similar execution cost. To achieve this, partitions may
need different spatial sizes.

\textbf{Security}. The system needs to be secure and resilient against
denial of service attacks.

\subsection{Open Issues}\label{open-issues}

What follows is a (non-exhaustive) list of issues, technical and
scientific, that are still being discussed and where changes are
possible.

\begin{itemize}
\item
  \textbf{Support for updates}. Size of Level 1 catalog is relatively
  small, and the expected query access patterns are relatively
  non-challenging, thus currently do not envision any need to deploy
  scalable Qserv-like architecture for Alert Production. Should this
  change, we will need to support updates in Qserv, which will likely
  have some non-trivial impact on the architecture.
\item
  \textbf{Very large objects}. Some objects (eg, large galaxies) are
  much larger than our overlap region, in some cases their footprint
  will span multiple chunks. Currently we are working with the object
  center, neglecting the actual footprint. While there are some science
  use cases that would benefit from a system that tracks objects based
  on their footprint, this is currently not a requirement. Potential
  solution would involve adding a custom index similar to the
  r-tree-based indexes such as the TOUCH \citep{Nobari:2013:TIS:2463676.2463700}.
\item
  \textbf{Very large results}. Currently, the front-end that disptached
  the query is responsible for assembling the results. In general, this
  is not a scalable approach as the resources required to processes the
  results may be several orders of magnitude greater than those needed
  to dispatch the query. One solution is to replicate the front-end to
  the extent necessary to handle query results. Alternatively, the
  Scalable Service Interface can be augmented to allow running
  disconnected queries. That is, once a particular front-end dispatches
  a query it can get a handle to that query and disconnect from it.
  Another server can, using that handle, reconnect to the query and
  process the results. This is a more flexible model as it allows
  independent scaling of query dispatch and result processing. It also
  has the aded benefit of not cancelling in-progress queries dispatched
  by a particulr fron-end should that front-end die.
\end{itemize}

\section{References}\label{references}
\renewcommand{\refname}{}
\bibliography{local,lsst,lsst-dm,refs_ads,refs,books}

\end{document}
